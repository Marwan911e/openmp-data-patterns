# Quick Reference Guide - OpenMP Performance

## üö´ Common Anti-Patterns (What NOT to Do)

### ‚ùå Anti-Pattern 1: Atomic in Hot Loop
```c
// BAD: Atomic on every iteration
#pragma omp parallel for
for (int i = 0; i < N; i++) {
    #pragma omp atomic
    shared_variable += data[i];  // ‚ùå N atomic operations!
}
```

### ‚úÖ Correct Pattern: Local Reduction
```c
// GOOD: Use local variables, combine at end
#pragma omp parallel
{
    int local_sum = 0;
    #pragma omp for nowait
    for (int i = 0; i < N; i++) {
        local_sum += data[i];  // ‚úÖ No synchronization
    }
    #pragma omp atomic
    shared_variable += local_sum;  // ‚úÖ Only 1 atomic per thread
}
```

---

### ‚ùå Anti-Pattern 2: Over-Parallelization
```c
// BAD: Parallelizing accumulation dimension
#pragma omp for collapse(3)
for (int i = 0; i < N; i++) {
    for (int j = 0; j < N; j++) {
        for (int k = 0; k < N; k++) {  // ‚ùå Multiple threads write to C[i][j]
            #pragma omp atomic
            C[i][j] += A[i][k] * B[k][j];
        }
    }
}
```

### ‚úÖ Correct Pattern: Parallelize Output Only
```c
// GOOD: Only parallelize independent dimensions
#pragma omp for collapse(2)
for (int i = 0; i < N; i++) {
    for (int j = 0; j < N; j++) {
        double sum = 0.0;
        for (int k = 0; k < N; k++) {  // ‚úÖ Sequential, no conflicts
            sum += A[i][k] * B[k][j];
        }
        C[i][j] = sum;  // ‚úÖ Each thread owns C[i][j]
    }
}
```

---

### ‚ùå Anti-Pattern 3: False Sharing
```c
// BAD: Adjacent threads write to nearby memory
int results[8];  // ‚ùå On same cache line
#pragma omp parallel
{
    int tid = omp_get_thread_num();
    results[tid] = compute();  // ‚ùå Cache line bouncing
}
```

### ‚úÖ Correct Pattern: Proper Alignment
```c
// GOOD: Pad to avoid false sharing
int results[8][16];  // ‚úÖ Each on different cache line
#pragma omp parallel
{
    int tid = omp_get_thread_num();
    results[tid][0] = compute();  // ‚úÖ No false sharing
}
```

---

## ‚úÖ Best Practices Checklist

### Before Parallelizing
- [ ] Identify independent work units
- [ ] Check for data dependencies
- [ ] Verify no shared writes in hot loops
- [ ] Choose appropriate decomposition strategy

### During Implementation
- [ ] Minimize synchronization points
- [ ] Use local variables in threads
- [ ] Combine results at the end only
- [ ] Choose correct scheduling policy

### After Implementation
- [ ] Test correctness vs sequential version
- [ ] Measure actual speedup
- [ ] Profile for bottlenecks
- [ ] Document performance characteristics

---

## üìä Scheduling Guide

### Static Scheduling
```c
#pragma omp for schedule(static)
```
**Use when:**
- Uniform workload (equal work per iteration)
- Memory-bound operations
- Want minimal overhead

**Examples:** Vector addition, dense matrix operations

### Dynamic Scheduling
```c
#pragma omp for schedule(dynamic, chunk_size)
```
**Use when:**
- Irregular workload (varying work per iteration)
- Unknown work distribution
- Load balancing important

**Examples:** Sparse matrices, tree traversal, irregular loops

### Guided Scheduling
```c
#pragma omp for schedule(guided)
```
**Use when:**
- Decreasing work per iteration
- Want dynamic benefits with less overhead

---

## üéØ Data Decomposition Strategies

### 1. Block Decomposition
**Best for:** Matrices, 2D data
```c
#pragma omp for collapse(2)
for (int bi = 0; bi < N; bi += BLOCK)
    for (int bj = 0; bj < N; bj += BLOCK)
        process_block(bi, bj);
```

### 2. Element Partitioning
**Best for:** Vectors, 1D arrays
```c
#pragma omp for
for (int i = 0; i < N; i++)
    C[i] = A[i] + B[i];
```

### 3. Row/Column Decomposition
**Best for:** Sparse matrices, irregular data
```c
#pragma omp for schedule(dynamic)
for (int row = 0; row < num_rows; row++)
    process_row(row);
```

---

## üîç Performance Debugging

### Why is my parallel code slower?

#### Problem: Negative Speedup
**Symptoms:** Parallel version slower than sequential

**Common Causes:**
1. ‚ùå Atomic/critical in hot loop ‚Üí Remove or use reduction
2. ‚ùå Input too small ‚Üí Use larger problem size
3. ‚ùå Memory-bound operation ‚Üí Limited by bandwidth, not CPU
4. ‚ùå Too many threads ‚Üí Reduce to match CPU cores

#### Problem: No Speedup
**Symptoms:** Parallel same speed as sequential

**Common Causes:**
1. ‚ùå Serialization bottleneck ‚Üí Check for implicit barriers
2. ‚ùå Load imbalance ‚Üí Try dynamic scheduling
3. ‚ùå False sharing ‚Üí Pad data structures
4. ‚ùå Memory bandwidth saturated ‚Üí Expected for memory-bound ops

#### Problem: Incorrect Results
**Symptoms:** Parallel gives different answer than sequential

**Common Causes:**
1. ‚ùå Race condition ‚Üí Add synchronization or restructure
2. ‚ùå Uninitialized shared data ‚Üí Use `shared` and `private` correctly
3. ‚ùå Missing reduction clause ‚Üí Use `reduction(+:var)`

---

## üí° Quick Fixes

### Fix 1: Reduction Pattern
```c
// Before: Atomic in loop
for (i) { #pragma omp atomic; sum += x[i]; }

// After: Use OpenMP reduction
#pragma omp parallel for reduction(+:sum)
for (i) { sum += x[i]; }
```

### Fix 2: Local Accumulation
```c
// Before: Shared counter
for (i) { #pragma omp atomic; histogram[data[i]]++; }

// After: Local histograms
int local_hist[BINS] = {0};
#pragma omp for
for (i) { local_hist[data[i]]++; }
#pragma omp critical
{ for (b) histogram[b] += local_hist[b]; }
```

### Fix 3: Proper Work Partitioning
```c
// Before: Parallelize all dimensions
#pragma omp for collapse(3)
for (i) for (j) for (k) { atomic update C[i][j]; }

// After: Parallelize output only
#pragma omp for collapse(2)
for (i) for (j) { sum=0; for (k) sum+=...; C[i][j]=sum; }
```

---

## üìè Performance Expectations

### Compute-Bound Operations
- **Examples:** Matrix multiplication, FFT
- **Expected:** Near-linear speedup (e.g., 8x on 8 cores)
- **Limit:** Number of CPU cores

### Memory-Bound Operations
- **Examples:** Vector addition, transpose, sparse matrix
- **Expected:** Modest speedup (2-8x)
- **Limit:** Memory bandwidth (typically 4-8 channels)

### I/O-Bound Operations
- **Examples:** File encryption, data loading
- **Expected:** Limited speedup (2-4x)
- **Limit:** Disk/network bandwidth

---

## üéì Key Principles

### 1. No Shared Writes in Hot Loops
> "If multiple threads need to update the same variable in a hot loop, you're doing it wrong."

### 2. Synchronization is Expensive
> "Every atomic/critical/barrier is a potential bottleneck. Minimize them."

### 3. Match Parallelism to Problem Size
> "Parallel overhead is fixed. Make the problem large enough to amortize it."

### 4. Memory Bandwidth is Real
> "You can't parallelize your way past memory bandwidth limits."

### 5. Profile Before Optimizing
> "Measure, don't guess. Profile to find real bottlenecks."

---

## üìö Task-Specific Tips

### Task 1: Matrix Multiplication
- ‚úÖ Parallelize (i,j), sequential k
- ‚úÖ Use blocking for cache efficiency
- ‚ùå Don't parallelize k dimension

### Task 3: Histogram
- ‚úÖ Use local histograms + reduction
- ‚úÖ Critical section for final merge
- ‚ùå Don't use atomic on every element

### Task 4: Transpose
- ‚úÖ Blocked for cache locality
- ‚úÖ Dynamic scheduling for edges
- Note: Memory-bound, limited speedup

### Task 5: Vector Addition
- ‚úÖ Static scheduling (uniform work)
- ‚úÖ Use large vectors (>10M elements)
- Note: Memory bandwidth limited

### Task 6: Sparse Matrix
- ‚úÖ Dynamic scheduling (irregular work)
- ‚úÖ Adaptive chunk size
- Note: Row-wise independence

---

## üîó Additional Resources

### OpenMP Documentation
- [OpenMP API Specification](https://www.openmp.org/specifications/)
- [OpenMP Best Practices](https://www.openmp.org/resources/tutorials-articles/)

### Performance Tools
- `time` - Basic timing
- `perf` - Linux performance profiler
- Intel VTune - Advanced profiling
- Valgrind Callgrind - Call graph profiling

### Books
- "Using OpenMP" by Chapman, Jost, and van der Pas
- "Parallel Programming in OpenMP" by Chandra et al.

---

## ‚ú® Final Checklist

Before submitting your parallel code:

- [ ] Correctness verified against sequential version
- [ ] No atomic/critical in hot loops
- [ ] Proper work decomposition (independent units)
- [ ] Appropriate scheduling policy chosen
- [ ] Tested with multiple problem sizes
- [ ] Documented performance characteristics
- [ ] Measured and reported speedup
- [ ] Code is clean and well-commented

---

**Remember:** Good parallel code requires thinking about the algorithm, not just adding `#pragma omp parallel`. Understanding data dependencies and work decomposition is essential!

